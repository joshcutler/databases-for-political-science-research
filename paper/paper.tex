\documentclass[]{article} 
\usepackage{../common}
\usepackage[usenames,dvipsnames]{color}
\usepackage[colorlinks=TRUE, urlcolor=NavyBlue,citecolor=NavyBlue]{hyperref}
\usepackage{paralist}

\begin{document}

% title with git version info
\maketitle

%%% These terms and topics should be included in the paper:
%%% - record
%%% - field
%%% - foreign key
%%% - query
%%% - schema
%%% - result set
%%% - cursor (maybe)
%%% - table
%%% - SQL
%%% - ACID/transactions
%%% - join

\section{Introduction}

With rows representing observations and columns representing variables, the rectangular
table is the root of most social science research. Tables are built into \R as
either \texttt{matrix} or \texttt{data.frame} objects (and other more
specialized versions). In \Stata, the table is so ingrained as to be
invisible. In both systems, these data objects form the primary input to
routines for description, analysis, and plotting. The frequent use of tables
drives how data are archived and publicly released. It is safe to say, most statistical analysis
is probably carried out on variants of \texttt{.csv} files (or proprietary
equivalents).

\begin{table}
  \centering
  \begin{tabular}{|l|l|l|l|l|}
  \hline
  Subject ID & Favors Treaty & Friends & Country & Signed Treaty \\\hline
  1          & 1             &         & USA     & 1             \\
  2          & 0             & 1       & USA     & 1             \\
  3          & 0             & 4       & Mexico  & 0             \\
  4          & 0             & 3       & Mexico  & 0             \\
  5          & 0             & 6,7     & Canada  & 1             \\
  6          & 1             & 5       & Canada  & 1             \\
  7          & 1             & 5       & Canada  & 1             \\
  \hline
\end{tabular}
\caption{Fictional data for a cross national survey asking citizens their
position on an international treaty. This table includes data on whether the
citizen's country signed the treaty.}
\label{tab:examplesurvey}
\end{table}
  
If the simple table and related formats have proved so popular, why should
social scientists consider alternative methods of storing and retrieving data?
We motivate our answer with the following example. Imagine a team of
researchers
conducting a multi-country survey. The team has data on country level
variables, for example whether the country signed a particular treaty, and
data on individual citizens, such as an attitude measure regarding the treaty.
Table~\ref{tab:examplesurvey} shows how these data are arranged in a
rectangular table. The rows represent individual responses, but also carry
country level variables, in this case treaty membership. Additionally, the
``Friends'' column indicates the result of a social network question in which
respondents are asked if they know other members of the survey (e.g.
respondent 1 does not list any friends, while respondent 5 knows and is known
by 6 and 7).

Imagine that during the course of analysis, it is realized that while Canada
signed the treaty, it did so \emph{after} they survey was in the field. For
each Canadian respondent, the treaty status has to be updated. Additionally,
the researchers wish to include additional data on each country,
necessitating updating every row, even though there are only a few countries in
the dataset. With collaborators at different institutions, it quickly becomes
onerous to ship around updated copies of the data. At one point, two
researchers mistakenly edit the data file at the same time, and one erases the
other's changes. After these issues are resolved, the researchers turn
attention to analyzing the network effects. With a variable number of friends,
restructuring the table to account for the connections is no easy task.
Additionally, analysis was going to be carried out using a specialized graph
analysis package written in Python, which had trouble reading the researchers'
\texttt{.dta} file. Again
the researchers must turn their attention from the substantive question to
solving data issues.

While none of the problems encountered by the researchers were insurmountable,
databases provide solutions to each.  In this paper, we encourage researchers
to consider using databases as an alternative to the simple rectangular table
in order to minimize time spent updating and combining data, provide for
sharing data safely, and properly and efficiently represent data that cross
row boundaries of typical tables.  In the remainder of this article we
describe several types of databases and provide examples of real world social
science usage that addresses the problems faced by our example researchers. In
the final section, we also highlight several features that databases, such as
speed advantages for large datasets and tools for geographic and spatial data. 


\section{Databases Types and Applications}

\subsection{Flat Files}

\subsubsection{Connecting survey responses and Census information}

An online survey experiment with a pool of Canadian citizens
generated data on respondents' perceptions of the racial, ethnic, and partisan
compositions of respondents' communities. Respondents were randomly assigned
to see a map that included a randomly assigned official political district,
from large Census Divisions down to small Dissemination Areas (the equivalent
of U.S. Census tracts). Researchers were interested to see if respondents were
more accurate in describing the true composition of small districts versus
large districts. Consequently, individual level responses had to be connected
to aggregate level information at the various census district levels.

Since the survey data and the census
data were both in a tabular format (\texttt{.csv}), the team of researchers
decided to build an ad-hoc database out of the flat files and use \R's
\texttt{merge} function to handle constructing a composite table from the
component pieces. First, each respondent was geocoded to generate a
latitude-longitude coordinate. Next, each of these points was compared with
shape files for the census districts to find the ID number for the districts
for each respondent (more details on this process can be found in
Section~\ref{sec:additionaluses}). At this point, there were two tables: the
survey responses in one table and a table of subject IDs and district IDs.
These tables were merged on the ID field. The results were then repeatedly
merged with the census data using the district ID fields.
Figure~\ref{fig:surveycensus} shows how the tables were connected.

[TODO: figure here]

With the composite table completed, researchers could analyze it using
standard \R tools, such as the \texttt{lm} function. Following the suggestions
in \citet{Fredrickson:2010fk}, the process by which the combined table was
created was written in a series of scripts (specifically, using \texttt{gnu
make}). If data at the subject level or census level had to be adjusted or
extended, the scripts could be re-run to generate a new composite table with
the appropriate updates.

\subsection{Relational Databases}

\subsubsection{Multi-language data interaction source}

On key advantage of a well known relational database is that it can be used as
an interaction point for tools written in different languages. If all the
tools can connect to the database, researchers do not need to worry about
transforming data from a format for one tool to the format for another tool.
As an example, researchers were building a collection of hate crime incidents
across the United States and connecting these incidents to information on the
counties in which they occurred. A relational database provided a place for
tools for web scraping, geocoding, and spatial analysis to interact, as well
as housing data at multiple levels in appropriate tables.

The Southern Poverty Law Center collects hate crime events from media reports
across the United States. At the time of this analysis, the results were plotted
on a map on their website
(\href{http://www.splcenter.org/get-informed/hate-incidents}{the data have subsequently been released as
\texttt{.csv} file}). By reading the page source of the map display page, the
researchers realized they could fetch the results as RSS files for each state
(an XML format often used to publish feeds from blogs).
To perform this task, a series of Ruby scripts were developed to scrape this
data and turn it into a convenient tabular format. These data were then
inserted into a relational database.

Each hate incident had to be geocoded in order to find the county in which the
event occurred. Using Google's geocoding service, a Clojure application was
written to geocode the incidents and match the point to the proper county
using shape files of every Unite States county. The FIPS code for the matched
county was inserted into the table of hate incidents.
The FIPS code was used to link the events table in the database to a table of
county information released by the U.S. Census. Census data was released in
tabular format, making it easy to add to the database. For the final analysis,
the researchers connected to the database using \R's \texttt{DBI} package and
created tables by issuing SQL commands.

\subsubsection{A small database using the \texttt{RSQLite} package}

\subsection{Key-value Databases}

\subsubsection{Flexibly storing survey questions and responses}

\subsubsection{Using the \texttt{RBerkeley} package}

\section{Additional Uses}
\label{sec:additionaluses}

%%% - spatial databases (postgis)
%%% - proprietary (e.g. saleforce, getactive, are there social science specific
%%%   data services?)
%%% - Supreme Court DB at WashU

\section{Conclusion}

\bibliographystyle{apalike}
\bibliography{/Users/mark/Documents/Papers/main}

\end{document}

