\documentclass[]{article} 
\usepackage{../common}
\usepackage[usenames,dvipsnames]{color}
\usepackage[colorlinks=TRUE, urlcolor=NavyBlue,citecolor=NavyBlue]{hyperref}
\usepackage{paralist}

\begin{document}

% title with git version info
\maketitle

%%% These terms and topics should be included in the paper:
%%% - record
%%% - field
%%% - foreign key
%%% - query
%%% - schema
%%% - result set
%%% - cursor (maybe)
%%% - table
%%% - SQL
%%% - ACID/transactions
%%% - join

\section{Introduction}

With rows representing observations and columns representing variables, the rectangular
table is the root of most social science research. Tables are built into \R as
either \texttt{matrix} or \texttt{data.frame} objects (and other more
specialized versions). In \Stata, the table is so ingrained as to be
invisible. In both systems, these data objects form the primary input to
routines for description, analysis, and plotting. The frequent use of tables
drives how data are archived and publicly released. It is safe to say, most statistical analysis
is probably carried out on variants of \texttt{.csv} files (or proprietary
equivalents).

\begin{table}
  \centering
  \begin{tabular}{|l|l|l|l|l|}
  \hline
  Subject ID & Favors Treaty & Friends & Country & Signed Treaty \\\hline
  1          & 1             &         & USA     & 1             \\
  2          & 0             & 1       & USA     & 1             \\
  3          & 0             & 4       & Mexico  & 0             \\
  4          & 0             & 3       & Mexico  & 0             \\
  5          & 0             & 6,7     & Canada  & 1             \\
  6          & 1             & 5       & Canada  & 1             \\
  7          & 1             & 5       & Canada  & 1             \\
  \hline
\end{tabular}
\caption{Fictional data for a cross national survey asking citizens their
position on an international treaty. This table includes data on whether the
citizen's country signed the treaty.}
\label{tab:examplesurvey}
\end{table}
  
If the simple table and related formats have proved so popular, why should
social scientists consider alternative methods of storing and retrieving data?
We motivate our answer with the following example. Imagine a team of
researchers
conducting a multi-country survey. The team has data on country level
variables, for example whether the country signed a particular treaty, and
data on individual citizens, such as an attitude measure regarding the treaty.
Table~\ref{tab:examplesurvey} shows how these data are arranged in a
rectangular table. The rows represent individual responses, but also carry
country level variables, in this case treaty membership. Additionally, the
``Friends'' column indicates the result of a social network question in which
respondents are asked if they know other members of the survey (e.g.
respondent 1 does not list any friends, while respondent 5 knows and is known
by 6 and 7).

Imagine that during the course of analysis, it is realized that while Canada
signed the treaty, it did so \emph{after} they survey was in the field. For
each Canadian respondent, the treaty status has to be updated. Additionally,
the researchers wish to include additional data on each country,
necessitating updating every row, even though there are only a few countries in
the dataset. With collaborators at different institutions, it quickly becomes
onerous to ship around updated copies of the data. At one point, two
researchers mistakenly edit the data file at the same time, and one erases the
other's changes. After these issues are resolved, the researchers turn
attention to analyzing the network effects. With a variable number of friends,
restructuring the table to account for the connections is no easy task.
Additionally, analysis was going to be carried out using a specialized graph
analysis package written in Python, which had trouble reading the researchers'
\texttt{.dta} file. Again
the researchers must turn their attention from the substantive question to
solving data issues.

While none of the problems encountered by the researchers were insurmountable,
databases provide solutions to each.  In this paper, we encourage researchers
to consider using databases as an alternative to the simple rectangular table
in order to minimize time spent updating and combining data, provide for
sharing data safely, and properly and efficiently represent data that cross
row boundaries of typical tables.  In the remainder of this article we
describe several types of databases and provide examples of real world social
science usage that addresses the problems faced by our example researchers. In
the final section, we also highlight several features that databases, such as
speed advantages for large datasets and tools for geographic and spatial data. 


\section{Databases Types and Applications}

\subsection{Flat Files}

\subsubsection{Connecting survey responses and Census information}
\label{sec:surveycensus}

An online survey experiment with a pool of Canadian citizens
generated data on respondents' perceptions of the racial, ethnic, and partisan
compositions of respondents' communities. Respondents were randomly assigned
to see a map that included a randomly assigned official political district,
from large Census Divisions down to small Dissemination Areas (the equivalent
of U.S. Census tracts). Researchers were interested to see if respondents were
more accurate in describing the true composition of small districts versus
large districts. Consequently, individual level responses had to be connected
to aggregate level information at the various census district levels.

Since the survey data and the census
data were both in a tabular format (\texttt{.csv}), the team of researchers
decided to build an ad-hoc database out of the flat files and use \R's
\texttt{merge} function to handle constructing a composite table from the
component pieces. First, each respondent was geocoded to generate a
latitude-longitude coordinate. Next, each of these points was compared with
shape files for the census districts to find the ID number for the districts
for each respondent (more details on this process can be found in
Section~\ref{sec:additionaluses}). At this point, there were two tables: the
survey responses in one table and a table of subject IDs and district IDs.
These tables were merged on the ID field. The results were then repeatedly
merged with the census data using the district ID fields.
Figure~\ref{fig:surveycensus} shows how the tables were connected.

[TODO: figure here]

With the composite table completed, researchers could analyze it using
standard \R tools, such as the \texttt{lm} function. Following the suggestions
in \citet{Fredrickson:2010fk}, the process by which the combined table was
created was written in a series of scripts (specifically, using \texttt{gnu
make}). If data at the subject level or census level had to be adjusted or
extended, the scripts could be re-run to generate a new composite table with
the appropriate updates.

\subsection{Relational Databases}

\subsubsection{Multi-language data interaction source}

On key advantage of a well known relational database is that it can be used as
an interaction point for tools written in different languages. If all the
tools can connect to the database, researchers do not need to worry about
transforming data from a format for one tool to the format for another tool.
As an example, researchers were building a collection of hate crime incidents
across the United States and connecting these incidents to information on the
counties in which they occurred. A relational database provided a place for
tools for web scraping, geocoding, and spatial analysis to interact, as well
as housing data at multiple levels in appropriate tables.

The Southern Poverty Law Center collects hate crime events from media reports
across the United States. At the time of this analysis, the results were plotted
on a map on their website
(\href{http://www.splcenter.org/get-informed/hate-incidents}{the data have subsequently been released as
\texttt{.csv} file}). By reading the page source of the map display page, the
researchers realized they could fetch the results as RSS files for each state
(an XML format often used to publish feeds from blogs).
To perform this task, a series of Ruby scripts were developed to scrape this
data and turn it into a convenient tabular format. These data were then
inserted into a relational database.

Each hate incident had to be geocoded in order to find the county in which the
event occurred. Using Google's geocoding service, a Clojure application was
written to geocode the incidents and match the point to the proper county
using shape files of every Unite States county. The FIPS code for the matched
county was inserted into the table of hate incidents.
The FIPS code was used to link the events table in the database to a table of
county information released by the U.S. Census. Census data was released in
tabular format, making it easy to add to the database. For the final analysis,
the researchers connected to the database using \R's \texttt{DBI} package and
created tables by issuing SQL commands.

\subsubsection{A small database using the \texttt{RSQLite} package}

\subsection{Key-value Databases}

\subsubsection{Flexibly storing survey questions and responses}

Researchers looking to run online surveys and survey experiments might first
look to a relational database to house data from respondents. At first, this
strategy seems to make sense. The goal is to collect data on respondents
(rows), with a series of responses (columns). If respondents are grouped in
some fashion, these groupings can be expressed by foreign keys in other tables
in the database. While this describes the outcome of the survey experiment, it
ignores the process of developing the survey.
In the process of writing
survey experiments, researchers will certainly add questions, delete
questions, and change the possible answers to questions. In most relational
databases, if the results for a question were stored in its own field, these
changes would require updating both the code handling the question prompts as
well as the database schema, the description of the tables that includes the
data types of the columns. These updates would be time consuming and provide
an opportunity for bugs to sneak in as the front end and back end code bases
might have different understandings of the data format.

\href{https://github.com/markmfredrickson/shanks}{Shanks} is a software
package for creating online survey experiments that takes a different
strategy. Instead of a relational database, Shanks uses a ``schema-free''
database.\footnote{While any key-value database would be able to provide this
service, Shanks currently only has an implementation that uses Google App
Engine's datastore service.} When respondents begin the survey, they are
assigned a unique ID. Any randomizations or immediate data are saved to the
database under their ID number. As they progress through the individual screens in the
survey, all previous answers are fetched and new answers saved. The database
is agnostic about the content fetched; simply returns all data for a given ID
number. During development, researchers could add or delete questions and the
changes would be immediate in the database. Additionally, based on
randomizations or previous answers, some respondents see different questions
than others. These differences do not pose any problem for the database.

Shanks was used for the survey discussed in Example~\ref{sec:surveycensus}.
For the purposes of analysis, the research team exported the survey responses
as a flat \texttt{.csv} file. The rows were the individual survey responses.
The columns were the union of all possible questions encountered by subjects.
For questions encountered by all subjects, the columns were filled in
entirely. For columns only encountered by some subjects, \texttt{NA} values
were inserted for subjects who did not see the question. By waiting until the
analysis stage to put the data in a rectangular format, the researchers were
able to very quickly create and edit the survey on a live test server, but
export it in a format that was easy to combine with Census data and use in a
simple ad-hoc database.

\subsubsection{Using the \texttt{RBerkeley} package}

\section{Additional Uses}
\label{sec:additionaluses}

%%% - spatial databases (postgis)
%%% - proprietary (e.g. saleforce, getactive, are there social science specific
%%%   data services?)
%%% - Supreme Court DB at WashU

\section{Conclusion}

\bibliographystyle{apalike}
\bibliography{/Users/mark/Documents/Papers/main}

\end{document}

